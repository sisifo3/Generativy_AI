{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#link\n",
        "#https://github.com/google-research/ALBERT/blob/master/run_classifier.py"
      ],
      "metadata": {
        "id": "NDkOtwH15_Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Team Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"BERT finetuning on classification tasks.\"\"\"\n"
      ],
      "metadata": {
        "id": "GvkW0pNV5n_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/google-research/albert.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5Ws6zVeMQo1",
        "outputId": "7c5ac4f0-14fb-4d06-faeb-83efe7fd1198"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'albert'...\n",
            "remote: Enumerating objects: 398, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 398 (delta 21), reused 22 (delta 12), pack-reused 353\u001b[K\n",
            "Receiving objects: 100% (398/398), 319.03 KiB | 6.02 MiB/s, done.\n",
            "Resolving deltas: 100% (253/253), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!dir\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPiWmHaXMkAW",
        "outputId": "ce737c1a-8240-402f-a9ba-f15edec51680"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "albert\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd albert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkdKmigxMzkT",
        "outputId": "2846800d-d128-45f0-bc7f-80ac45c04f54"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/albert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "notggpsvNJ67",
        "outputId": "c15f9611-21a1-4b34-9bfc-cc75e4e82863"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/albert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja3pOCUwM2E9",
        "outputId": "45817cc0-4142-4b72-c2b2-cd977b82838c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "albert_glue_fine_tuning_tutorial.ipynb\tmodeling.py\t      run_pretraining_test.py\n",
            "classifier_utils.py\t\t\tmodeling_test.py      run_race.py\n",
            "CONTRIBUTING.md\t\t\t\toptimization.py       run_squad_v1.py\n",
            "create_pretraining_data.py\t\toptimization_test.py  run_squad_v2.py\n",
            "export_checkpoints.py\t\t\trace_utils.py\t      run_trivial_model_test.sh\n",
            "export_to_tfhub.py\t\t\tREADME.md\t      squad_utils.py\n",
            "fine_tuning_utils.py\t\t\trequirements.txt      tokenization.py\n",
            "__init__.py\t\t\t\trun_classifier.py     tokenization_test.py\n",
            "lamb_optimizer.py\t\t\trun_glue.sh\n",
            "LICENSE\t\t\t\t\trun_pretraining.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd albert\n",
        "!dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-22EcJRXM5TS",
        "outputId": "4e16e00b-9564-4564-d458-87c7cbacf4d0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "albert\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbK4x_Wc_xPH",
        "outputId": "a785f943-551e-4763-d71a-e358fa963595"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albert\n",
            "  Downloading albert-1.3.1.tar.gz (3.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from albert) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->albert) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->albert) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->albert) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->albert) (2024.2.2)\n",
            "Building wheels for collected packages: albert\n",
            "  Building wheel for albert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albert: filename=albert-1.3.1-py3-none-any.whl size=3817 sha256=6c51aaee6da524ad2f8dfe601de7f8c8764bc86e299a0638468ae64e703cf07e\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/ef/46/93b723d465e753164cf24475972458e78445326c186033fd38\n",
            "Successfully built albert\n",
            "Installing collected packages: albert\n",
            "Successfully installed albert-1.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import albert"
      ],
      "metadata": {
        "id": "pEGHnCcyNlOo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "albert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjYMycO8NqNm",
        "outputId": "8692589d-1147-4c7e-95b6-5695701198f4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'albert' from '/usr/local/lib/python3.10/dist-packages/albert/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "from albert import classifier_utils\n",
        "from albert import fine_tuning_utils\n",
        "from albert import modeling\n",
        "import tensorflow.compat.v1 as tf\n",
        "from tensorflow.compat.v1 import estimator as tf_estimator\n",
        "from tensorflow.contrib import cluster_resolver as contrib_cluster_resolver\n",
        "from tensorflow.contrib import tpu as contrib_tpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "puZkHNbY5uJ_",
        "outputId": "01f90e29-ea7e-4b3a-9bf5-9cd6e3dc57c4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'classifier_utils' from 'albert' (/usr/local/lib/python3.10/dist-packages/albert/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-50714610d30c>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0malbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassifier_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0malbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfine_tuning_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0malbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'classifier_utils' from 'albert' (/usr/local/lib/python3.10/dist-packages/albert/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "L43A1hcM2NhL",
        "outputId": "38e0b287-c287-434d-af9b-dcb927856361"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'albert'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-84314af18f0d>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0malbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassifier_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0malbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfine_tuning_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0malbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'albert'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "flags = tf.flags\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "## Required parameters\n",
        "flags.DEFINE_string(\n",
        "    \"data_dir\", None,\n",
        "    \"The input data dir. Should contain the .tsv files (or other data files) \"\n",
        "    \"for the task.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"albert_config_file\", None,\n",
        "    \"The config json file corresponding to the pre-trained ALBERT model. \"\n",
        "    \"This specifies the model architecture.\")\n",
        "\n",
        "flags.DEFINE_string(\"task_name\", None, \"The name of the task to train.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"vocab_file\", None,\n",
        "    \"The vocabulary file that the ALBERT model was trained on.\")\n",
        "\n",
        "flags.DEFINE_string(\"spm_model_file\", None,\n",
        "                    \"The model file for sentence piece tokenization.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"output_dir\", None,\n",
        "    \"The output directory where the model checkpoints will be written.\")\n",
        "\n",
        "flags.DEFINE_string(\"cached_dir\", None,\n",
        "                    \"Path to cached training and dev tfrecord file. \"\n",
        "                    \"The file will be generated if not exist.\")\n",
        "\n",
        "## Other parameters\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"init_checkpoint\", None,\n",
        "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"albert_hub_module_handle\", None,\n",
        "    \"If set, the ALBERT hub module to use.\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"do_lower_case\", True,\n",
        "    \"Whether to lower case the input text. Should be True for uncased \"\n",
        "    \"models and False for cased models.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"max_seq_length\", 512,\n",
        "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
        "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
        "    \"than this will be padded.\")\n",
        "\n",
        "flags.DEFINE_bool(\"do_train\", False, \"Whether to run training.\")\n",
        "\n",
        "flags.DEFINE_bool(\"do_eval\", False, \"Whether to run eval on the dev set.\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"do_predict\", False,\n",
        "    \"Whether to run the model in inference mode on the test set.\")\n",
        "\n",
        "flags.DEFINE_integer(\"train_batch_size\", 32, \"Total batch size for training.\")\n",
        "\n",
        "flags.DEFINE_integer(\"eval_batch_size\", 8, \"Total batch size for eval.\")\n",
        "\n",
        "flags.DEFINE_integer(\"predict_batch_size\", 8, \"Total batch size for predict.\")\n",
        "\n",
        "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
        "\n",
        "flags.DEFINE_integer(\"train_step\", 1000,\n",
        "                     \"Total number of training steps to perform.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"warmup_step\", 0,\n",
        "    \"number of steps to perform linear learning rate warmup for.\")\n",
        "\n",
        "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
        "                     \"How often to save the model checkpoint.\")\n",
        "\n",
        "flags.DEFINE_integer(\"keep_checkpoint_max\", 5,\n",
        "                     \"How many checkpoints to keep.\")\n",
        "\n",
        "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
        "                     \"How many steps to make in each estimator call.\")\n",
        "\n",
        "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
        "\n",
        "flags.DEFINE_string(\"optimizer\", \"adamw\", \"Optimizer to use\")\n",
        "\n",
        "tf.flags.DEFINE_string(\n",
        "    \"tpu_name\", None,\n",
        "    \"The Cloud TPU to use for training. This should be either the name \"\n",
        "    \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\n",
        "    \"url.\")\n",
        "\n",
        "tf.flags.DEFINE_string(\n",
        "    \"tpu_zone\", None,\n",
        "    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n",
        "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
        "    \"metadata.\")\n",
        "\n",
        "tf.flags.DEFINE_string(\n",
        "    \"gcp_project\", None,\n",
        "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
        "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
        "    \"metadata.\")\n",
        "\n",
        "tf.flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"num_tpu_cores\", 8,\n",
        "    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"export_dir\", None,\n",
        "    \"The directory where the exported SavedModel will be stored.\")\n",
        "\n",
        "flags.DEFINE_float(\n",
        "    \"threshold_to_export\", float(\"nan\"),\n",
        "    \"The threshold value that should be used with the exported classifier. \"\n",
        "    \"When specified, the threshold will be attached to the exported \"\n",
        "    \"SavedModel, and served along with the predictions. Please use the \"\n",
        "    \"saved model cli (\"\n",
        "    \"https://www.tensorflow.org/guide/saved_model#details_of_the_savedmodel_command_line_interface\"\n",
        "    \") to view the output signature of the threshold.\")\n",
        "\n",
        "\n",
        "def _serving_input_receiver_fn():\n",
        "  \"\"\"Creates an input function for serving.\"\"\"\n",
        "  seq_len = FLAGS.max_seq_length\n",
        "  serialized_example = tf.placeholder(\n",
        "      dtype=tf.string, shape=[None], name=\"serialized_example\")\n",
        "  features = {\n",
        "      \"input_ids\": tf.FixedLenFeature([seq_len], dtype=tf.int64),\n",
        "      \"input_mask\": tf.FixedLenFeature([seq_len], dtype=tf.int64),\n",
        "      \"segment_ids\": tf.FixedLenFeature([seq_len], dtype=tf.int64),\n",
        "  }\n",
        "  feature_map = tf.parse_example(serialized_example, features=features)\n",
        "  feature_map[\"is_real_example\"] = tf.constant(1, dtype=tf.int32)\n",
        "  feature_map[\"label_ids\"] = tf.constant(0, dtype=tf.int32)\n",
        "\n",
        "  # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
        "  # So cast all int64 to int32.\n",
        "  for name in feature_map.keys():\n",
        "    t = feature_map[name]\n",
        "    if t.dtype == tf.int64:\n",
        "      t = tf.to_int32(t)\n",
        "    feature_map[name] = t\n",
        "\n",
        "  return tf_estimator.export.ServingInputReceiver(\n",
        "      features=feature_map, receiver_tensors=serialized_example)\n",
        "\n",
        "\n",
        "def _add_threshold_to_model_fn(model_fn, threshold):\n",
        "  \"\"\"Adds the classifier threshold to the given model_fn.\"\"\"\n",
        "\n",
        "  def new_model_fn(features, labels, mode, params):\n",
        "    spec = model_fn(features, labels, mode, params)\n",
        "    threshold_tensor = tf.constant(threshold, dtype=tf.float32)\n",
        "    default_serving_export = spec.export_outputs[\n",
        "        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
        "    default_serving_export.outputs[\"threshold\"] = threshold_tensor\n",
        "    return spec\n",
        "\n",
        "  return new_model_fn\n",
        "\n",
        "\n",
        "def main(_):\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "  processors = {\n",
        "      \"cola\": classifier_utils.ColaProcessor,\n",
        "      \"mnli\": classifier_utils.MnliProcessor,\n",
        "      \"mismnli\": classifier_utils.MisMnliProcessor,\n",
        "      \"mrpc\": classifier_utils.MrpcProcessor,\n",
        "      \"rte\": classifier_utils.RteProcessor,\n",
        "      \"sst-2\": classifier_utils.Sst2Processor,\n",
        "      \"sts-b\": classifier_utils.StsbProcessor,\n",
        "      \"qqp\": classifier_utils.QqpProcessor,\n",
        "      \"qnli\": classifier_utils.QnliProcessor,\n",
        "      \"wnli\": classifier_utils.WnliProcessor,\n",
        "  }\n",
        "\n",
        "  if not (FLAGS.do_train or FLAGS.do_eval or FLAGS.do_predict or\n",
        "          FLAGS.export_dir):\n",
        "    raise ValueError(\n",
        "        \"At least one of `do_train`, `do_eval`, `do_predict' or `export_dir` \"\n",
        "        \"must be True.\")\n",
        "\n",
        "  if not FLAGS.albert_config_file and not FLAGS.albert_hub_module_handle:\n",
        "    raise ValueError(\"At least one of `--albert_config_file` and \"\n",
        "                     \"`--albert_hub_module_handle` must be set\")\n",
        "\n",
        "  if FLAGS.albert_config_file:\n",
        "    albert_config = modeling.AlbertConfig.from_json_file(\n",
        "        FLAGS.albert_config_file)\n",
        "    if FLAGS.max_seq_length > albert_config.max_position_embeddings:\n",
        "      raise ValueError(\n",
        "          \"Cannot use sequence length %d because the ALBERT model \"\n",
        "          \"was only trained up to sequence length %d\" %\n",
        "          (FLAGS.max_seq_length, albert_config.max_position_embeddings))\n",
        "  else:\n",
        "    albert_config = None  # Get the config from TF-Hub.\n",
        "\n",
        "  tf.gfile.MakeDirs(FLAGS.output_dir)\n",
        "\n",
        "  task_name = FLAGS.task_name.lower()\n",
        "\n",
        "  if task_name not in processors:\n",
        "    raise ValueError(\"Task not found: %s\" % (task_name))\n",
        "\n",
        "  processor = processors[task_name](\n",
        "      use_spm=True if FLAGS.spm_model_file else False,\n",
        "      do_lower_case=FLAGS.do_lower_case)\n",
        "\n",
        "  label_list = processor.get_labels()\n",
        "\n",
        "  tokenizer = fine_tuning_utils.create_vocab(\n",
        "      vocab_file=FLAGS.vocab_file,\n",
        "      do_lower_case=FLAGS.do_lower_case,\n",
        "      spm_model_file=FLAGS.spm_model_file,\n",
        "      hub_module=FLAGS.albert_hub_module_handle)\n",
        "\n",
        "  tpu_cluster_resolver = None\n",
        "  if FLAGS.use_tpu and FLAGS.tpu_name:\n",
        "    tpu_cluster_resolver = contrib_cluster_resolver.TPUClusterResolver(\n",
        "        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
        "\n",
        "  is_per_host = contrib_tpu.InputPipelineConfig.PER_HOST_V2\n",
        "  if FLAGS.do_train:\n",
        "    iterations_per_loop = int(min(FLAGS.iterations_per_loop,\n",
        "                                  FLAGS.save_checkpoints_steps))\n",
        "  else:\n",
        "    iterations_per_loop = FLAGS.iterations_per_loop\n",
        "  run_config = contrib_tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      master=FLAGS.master,\n",
        "      model_dir=FLAGS.output_dir,\n",
        "      save_checkpoints_steps=int(FLAGS.save_checkpoints_steps),\n",
        "      keep_checkpoint_max=0,\n",
        "      tpu_config=contrib_tpu.TPUConfig(\n",
        "          iterations_per_loop=iterations_per_loop,\n",
        "          num_shards=FLAGS.num_tpu_cores,\n",
        "          per_host_input_for_training=is_per_host))\n",
        "\n",
        "  train_examples = None\n",
        "  if FLAGS.do_train:\n",
        "    train_examples = processor.get_train_examples(FLAGS.data_dir)\n",
        "  model_fn = classifier_utils.model_fn_builder(\n",
        "      albert_config=albert_config,\n",
        "      num_labels=len(label_list),\n",
        "      init_checkpoint=FLAGS.init_checkpoint,\n",
        "      learning_rate=FLAGS.learning_rate,\n",
        "      num_train_steps=FLAGS.train_step,\n",
        "      num_warmup_steps=FLAGS.warmup_step,\n",
        "      use_tpu=FLAGS.use_tpu,\n",
        "      use_one_hot_embeddings=FLAGS.use_tpu,\n",
        "      task_name=task_name,\n",
        "      hub_module=FLAGS.albert_hub_module_handle,\n",
        "      optimizer=FLAGS.optimizer)\n",
        "\n",
        "  if not math.isnan(FLAGS.threshold_to_export):\n",
        "    model_fn = _add_threshold_to_model_fn(model_fn, FLAGS.threshold_to_export)\n",
        "\n",
        "  # If TPU is not available, this will fall back to normal Estimator on CPU\n",
        "  # or GPU.\n",
        "  estimator = contrib_tpu.TPUEstimator(\n",
        "      use_tpu=FLAGS.use_tpu,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=FLAGS.train_batch_size,\n",
        "      eval_batch_size=FLAGS.eval_batch_size,\n",
        "      predict_batch_size=FLAGS.predict_batch_size,\n",
        "      export_to_tpu=False)  # http://yaqs/4707241341091840\n",
        "\n",
        "  if FLAGS.do_train:\n",
        "    cached_dir = FLAGS.cached_dir\n",
        "    if not cached_dir:\n",
        "      cached_dir = FLAGS.output_dir\n",
        "    train_file = os.path.join(cached_dir, task_name + \"_train.tf_record\")\n",
        "    if not tf.gfile.Exists(train_file):\n",
        "      classifier_utils.file_based_convert_examples_to_features(\n",
        "          train_examples, label_list, FLAGS.max_seq_length, tokenizer,\n",
        "          train_file, task_name)\n",
        "    tf.logging.info(\"***** Running training *****\")\n",
        "    tf.logging.info(\"  Num examples = %d\", len(train_examples))\n",
        "    tf.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\n",
        "    tf.logging.info(\"  Num steps = %d\", FLAGS.train_step)\n",
        "    train_input_fn = classifier_utils.file_based_input_fn_builder(\n",
        "        input_file=train_file,\n",
        "        seq_length=FLAGS.max_seq_length,\n",
        "        is_training=True,\n",
        "        drop_remainder=True,\n",
        "        task_name=task_name,\n",
        "        use_tpu=FLAGS.use_tpu,\n",
        "        bsz=FLAGS.train_batch_size)\n",
        "    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.train_step)\n",
        "\n",
        "  if FLAGS.do_eval:\n",
        "    eval_examples = processor.get_dev_examples(FLAGS.data_dir)\n",
        "    num_actual_eval_examples = len(eval_examples)\n",
        "    if FLAGS.use_tpu:\n",
        "      # TPU requires a fixed batch size for all batches, therefore the number\n",
        "      # of examples must be a multiple of the batch size, or else examples\n",
        "      # will get dropped. So we pad with fake examples which are ignored\n",
        "      # later on. These do NOT count towards the metric (all tf.metrics\n",
        "      # support a per-instance weight, and these get a weight of 0.0).\n",
        "      while len(eval_examples) % FLAGS.eval_batch_size != 0:\n",
        "        eval_examples.append(classifier_utils.PaddingInputExample())\n",
        "\n",
        "    cached_dir = FLAGS.cached_dir\n",
        "    if not cached_dir:\n",
        "      cached_dir = FLAGS.output_dir\n",
        "    eval_file = os.path.join(cached_dir, task_name + \"_eval.tf_record\")\n",
        "    if not tf.gfile.Exists(eval_file):\n",
        "      classifier_utils.file_based_convert_examples_to_features(\n",
        "          eval_examples, label_list, FLAGS.max_seq_length, tokenizer,\n",
        "          eval_file, task_name)\n",
        "\n",
        "    tf.logging.info(\"***** Running evaluation *****\")\n",
        "    tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
        "                    len(eval_examples), num_actual_eval_examples,\n",
        "                    len(eval_examples) - num_actual_eval_examples)\n",
        "    tf.logging.info(\"  Batch size = %d\", FLAGS.eval_batch_size)\n",
        "\n",
        "    # This tells the estimator to run through the entire set.\n",
        "    eval_steps = None\n",
        "    # However, if running eval on the TPU, you will need to specify the\n",
        "    # number of steps.\n",
        "    if FLAGS.use_tpu:\n",
        "      assert len(eval_examples) % FLAGS.eval_batch_size == 0\n",
        "      eval_steps = int(len(eval_examples) // FLAGS.eval_batch_size)\n",
        "\n",
        "    eval_drop_remainder = True if FLAGS.use_tpu else False\n",
        "    eval_input_fn = classifier_utils.file_based_input_fn_builder(\n",
        "        input_file=eval_file,\n",
        "        seq_length=FLAGS.max_seq_length,\n",
        "        is_training=False,\n",
        "        drop_remainder=eval_drop_remainder,\n",
        "        task_name=task_name,\n",
        "        use_tpu=FLAGS.use_tpu,\n",
        "        bsz=FLAGS.eval_batch_size)\n",
        "\n",
        "    best_trial_info_file = os.path.join(FLAGS.output_dir, \"best_trial.txt\")\n",
        "\n",
        "    def _best_trial_info():\n",
        "      \"\"\"Returns information about which checkpoints have been evaled so far.\"\"\"\n",
        "      if tf.gfile.Exists(best_trial_info_file):\n",
        "        with tf.gfile.GFile(best_trial_info_file, \"r\") as best_info:\n",
        "          global_step, best_metric_global_step, metric_value = (\n",
        "              best_info.read().split(\":\"))\n",
        "          global_step = int(global_step)\n",
        "          best_metric_global_step = int(best_metric_global_step)\n",
        "          metric_value = float(metric_value)\n",
        "      else:\n",
        "        metric_value = -1\n",
        "        best_metric_global_step = -1\n",
        "        global_step = -1\n",
        "      tf.logging.info(\n",
        "          \"Best trial info: Step: %s, Best Value Step: %s, \"\n",
        "          \"Best Value: %s\", global_step, best_metric_global_step, metric_value)\n",
        "      return global_step, best_metric_global_step, metric_value\n",
        "\n",
        "    def _remove_checkpoint(checkpoint_path):\n",
        "      for ext in [\"meta\", \"data-00000-of-00001\", \"index\"]:\n",
        "        src_ckpt = checkpoint_path + \".{}\".format(ext)\n",
        "        tf.logging.info(\"removing {}\".format(src_ckpt))\n",
        "        tf.gfile.Remove(src_ckpt)\n",
        "\n",
        "    def _find_valid_cands(curr_step):\n",
        "      filenames = tf.gfile.ListDirectory(FLAGS.output_dir)\n",
        "      candidates = []\n",
        "      for filename in filenames:\n",
        "        if filename.endswith(\".index\"):\n",
        "          ckpt_name = filename[:-6]\n",
        "          idx = ckpt_name.split(\"-\")[-1]\n",
        "          if int(idx) > curr_step:\n",
        "            candidates.append(filename)\n",
        "      return candidates\n",
        "\n",
        "    output_eval_file = os.path.join(FLAGS.output_dir, \"eval_results.txt\")\n",
        "\n",
        "    if task_name == \"sts-b\":\n",
        "      key_name = \"pearson\"\n",
        "    elif task_name == \"cola\":\n",
        "      key_name = \"matthew_corr\"\n",
        "    else:\n",
        "      key_name = \"eval_accuracy\"\n",
        "\n",
        "    global_step, best_perf_global_step, best_perf = _best_trial_info()\n",
        "    writer = tf.gfile.GFile(output_eval_file, \"w\")\n",
        "    while global_step < FLAGS.train_step:\n",
        "      steps_and_files = {}\n",
        "      filenames = tf.gfile.ListDirectory(FLAGS.output_dir)\n",
        "      for filename in filenames:\n",
        "        if filename.endswith(\".index\"):\n",
        "          ckpt_name = filename[:-6]\n",
        "          cur_filename = os.path.join(FLAGS.output_dir, ckpt_name)\n",
        "          if cur_filename.split(\"-\")[-1] == \"best\":\n",
        "            continue\n",
        "          gstep = int(cur_filename.split(\"-\")[-1])\n",
        "          if gstep not in steps_and_files:\n",
        "            tf.logging.info(\"Add {} to eval list.\".format(cur_filename))\n",
        "            steps_and_files[gstep] = cur_filename\n",
        "      tf.logging.info(\"found {} files.\".format(len(steps_and_files)))\n",
        "      if not steps_and_files:\n",
        "        tf.logging.info(\"found 0 file, global step: {}. Sleeping.\"\n",
        "                        .format(global_step))\n",
        "        time.sleep(60)\n",
        "      else:\n",
        "        for checkpoint in sorted(steps_and_files.items()):\n",
        "          step, checkpoint_path = checkpoint\n",
        "          if global_step >= step:\n",
        "            if (best_perf_global_step != step and\n",
        "                len(_find_valid_cands(step)) > 1):\n",
        "              _remove_checkpoint(checkpoint_path)\n",
        "            continue\n",
        "          result = estimator.evaluate(\n",
        "              input_fn=eval_input_fn,\n",
        "              steps=eval_steps,\n",
        "              checkpoint_path=checkpoint_path)\n",
        "          global_step = result[\"global_step\"]\n",
        "          tf.logging.info(\"***** Eval results *****\")\n",
        "          for key in sorted(result.keys()):\n",
        "            tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
        "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "          writer.write(\"best = {}\\n\".format(best_perf))\n",
        "          if result[key_name] > best_perf:\n",
        "            best_perf = result[key_name]\n",
        "            best_perf_global_step = global_step\n",
        "          elif len(_find_valid_cands(global_step)) > 1:\n",
        "            _remove_checkpoint(checkpoint_path)\n",
        "          writer.write(\"=\" * 50 + \"\\n\")\n",
        "          writer.flush()\n",
        "          with tf.gfile.GFile(best_trial_info_file, \"w\") as best_info:\n",
        "            best_info.write(\"{}:{}:{}\".format(\n",
        "                global_step, best_perf_global_step, best_perf))\n",
        "    writer.close()\n",
        "\n",
        "    for ext in [\"meta\", \"data-00000-of-00001\", \"index\"]:\n",
        "      src_ckpt = \"model.ckpt-{}.{}\".format(best_perf_global_step, ext)\n",
        "      tgt_ckpt = \"model.ckpt-best.{}\".format(ext)\n",
        "      tf.logging.info(\"saving {} to {}\".format(src_ckpt, tgt_ckpt))\n",
        "      tf.io.gfile.rename(\n",
        "          os.path.join(FLAGS.output_dir, src_ckpt),\n",
        "          os.path.join(FLAGS.output_dir, tgt_ckpt),\n",
        "          overwrite=True)\n",
        "\n",
        "  if FLAGS.do_predict:\n",
        "    predict_examples = processor.get_test_examples(FLAGS.data_dir)\n",
        "    num_actual_predict_examples = len(predict_examples)\n",
        "    if FLAGS.use_tpu:\n",
        "      # TPU requires a fixed batch size for all batches, therefore the number\n",
        "      # of examples must be a multiple of the batch size, or else examples\n",
        "      # will get dropped. So we pad with fake examples which are ignored\n",
        "      # later on.\n",
        "      while len(predict_examples) % FLAGS.predict_batch_size != 0:\n",
        "        predict_examples.append(classifier_utils.PaddingInputExample())\n",
        "\n",
        "    predict_file = os.path.join(FLAGS.output_dir, \"predict.tf_record\")\n",
        "    classifier_utils.file_based_convert_examples_to_features(\n",
        "        predict_examples, label_list,\n",
        "        FLAGS.max_seq_length, tokenizer,\n",
        "        predict_file, task_name)\n",
        "\n",
        "    tf.logging.info(\"***** Running prediction*****\")\n",
        "    tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
        "                    len(predict_examples), num_actual_predict_examples,\n",
        "                    len(predict_examples) - num_actual_predict_examples)\n",
        "    tf.logging.info(\"  Batch size = %d\", FLAGS.predict_batch_size)\n",
        "\n",
        "    predict_drop_remainder = True if FLAGS.use_tpu else False\n",
        "    predict_input_fn = classifier_utils.file_based_input_fn_builder(\n",
        "        input_file=predict_file,\n",
        "        seq_length=FLAGS.max_seq_length,\n",
        "        is_training=False,\n",
        "        drop_remainder=predict_drop_remainder,\n",
        "        task_name=task_name,\n",
        "        use_tpu=FLAGS.use_tpu,\n",
        "        bsz=FLAGS.predict_batch_size)\n",
        "\n",
        "    checkpoint_path = os.path.join(FLAGS.output_dir, \"model.ckpt-best\")\n",
        "    result = estimator.predict(\n",
        "        input_fn=predict_input_fn,\n",
        "        checkpoint_path=checkpoint_path)\n",
        "\n",
        "    output_predict_file = os.path.join(FLAGS.output_dir, \"test_results.tsv\")\n",
        "    output_submit_file = os.path.join(FLAGS.output_dir, \"submit_results.tsv\")\n",
        "    with tf.gfile.GFile(output_predict_file, \"w\") as pred_writer,\\\n",
        "        tf.gfile.GFile(output_submit_file, \"w\") as sub_writer:\n",
        "      sub_writer.write(\"index\" + \"\\t\" + \"prediction\\n\")\n",
        "      num_written_lines = 0\n",
        "      tf.logging.info(\"***** Predict results *****\")\n",
        "      for (i, (example, prediction)) in\\\n",
        "          enumerate(zip(predict_examples, result)):\n",
        "        probabilities = prediction[\"probabilities\"]\n",
        "        if i >= num_actual_predict_examples:\n",
        "          break\n",
        "        output_line = \"\\t\".join(\n",
        "            str(class_probability)\n",
        "            for class_probability in probabilities) + \"\\n\"\n",
        "        pred_writer.write(output_line)\n",
        "\n",
        "        if task_name != \"sts-b\":\n",
        "          actual_label = label_list[int(prediction[\"predictions\"])]\n",
        "        else:\n",
        "          actual_label = str(prediction[\"predictions\"])\n",
        "        sub_writer.write(example.guid + \"\\t\" + actual_label + \"\\n\")\n",
        "        num_written_lines += 1\n",
        "    assert num_written_lines == num_actual_predict_examples\n",
        "\n",
        "  if FLAGS.export_dir:\n",
        "    tf.gfile.MakeDirs(FLAGS.export_dir)\n",
        "    checkpoint_path = os.path.join(FLAGS.output_dir, \"model.ckpt-best\")\n",
        "    tf.logging.info(\"Starting to export model.\")\n",
        "    subfolder = estimator.export_saved_model(\n",
        "        export_dir_base=FLAGS.export_dir,\n",
        "        serving_input_receiver_fn=_serving_input_receiver_fn,\n",
        "        checkpoint_path=checkpoint_path)\n",
        "    tf.logging.info(\"Model exported to %s.\", subfolder)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  flags.mark_flag_as_required(\"data_dir\")\n",
        "  flags.mark_flag_as_required(\"task_name\")\n",
        "  flags.mark_flag_as_required(\"spm_model_file\")\n",
        "  flags.mark_flag_as_required(\"output_dir\")\n",
        "  tf.app.run()"
      ]
    }
  ]
}