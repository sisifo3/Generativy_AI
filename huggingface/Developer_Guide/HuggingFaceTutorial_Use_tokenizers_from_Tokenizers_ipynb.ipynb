{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The PreTrainedTokenizerFast depends on the ðŸ¤— Tokenizers library. The tokenizers obtained from the ðŸ¤— Tokenizers library can be loaded very simply into ðŸ¤— Transformers.\n",
        "\n",
        "Before getting in the specifics, letâ€™s first start by creating a dummy tokenizer in a few lines:"
      ],
      "metadata": {
        "id": "L7IZCrG9A4pS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pvk1twxq3X7D"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "files = [\"/content/drive/MyDrive/tweets_ase.txt\"]\n",
        "tokenizer.train(files, trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a tokenizer trained on the files we defined. We can either continue using it in that runtime, or save it to a JSON file for future re-use.\n",
        "Loading directly from the tokenizer object\n",
        "\n",
        "Letâ€™s see how to leverage this tokenizer object in the ðŸ¤— Transformers library. The PreTrainedTokenizerFast class allows for easy instantiation, by accepting the instantiated tokenizer object as an argument:"
      ],
      "metadata": {
        "id": "3wsWDPtDEEJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)"
      ],
      "metadata": {
        "id": "62c2mOMJEEyt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This object can now be used with all the methods shared by the ðŸ¤— Transformers tokenizers! Head to the tokenizer page for more information.\n",
        "Loading from a JSON file\n",
        "\n",
        "In order to load a tokenizer from a JSON file, letâ€™s first start by saving our tokenizer:"
      ],
      "metadata": {
        "id": "-9-ARMNZEIXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save(\"tokenizer.json\")"
      ],
      "metadata": {
        "id": "otPxkMuQEJqu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The path to which we saved this file can be passed to the PreTrainedTokenizerFast initialization method using the tokenizer_file parameter:"
      ],
      "metadata": {
        "id": "DJn_n6n3ENQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\")"
      ],
      "metadata": {
        "id": "0qHaZr9kEOeS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This object can now be used with all the methods shared by the ðŸ¤— Transformers tokenizers! Head to the tokenizer page for more information."
      ],
      "metadata": {
        "id": "GYV7o04fEpYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fast_tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4DOiy57FpTy",
        "outputId": "155b2025-28d5-44b6-9d73-0d4a62b9006e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreTrainedTokenizerFast(name_or_path='', vocab_size=30000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t3: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "-wkmwMJ3IzRa",
        "outputId": "8344f7e2-c5c5-464f-858d-c867937cc53c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'PreTrainedTokenizerFast' object has no attribute 'items'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6605533c7a14>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfast_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'PreTrainedTokenizerFast' object has no attribute 'items'"
          ]
        }
      ]
    }
  ]
}